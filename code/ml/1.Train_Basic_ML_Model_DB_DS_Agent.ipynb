{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2a43d4-5ff3-43e9-ab14-640cc78e6af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use the Data Science Agent\n",
    "https://docs.databricks.com/aws/en/notebooks/ds-agent\n",
    "\n",
    "Machine learning:\n",
    "\"Perform some data preparation and feature engineering to prepare this dataset for model training.\"\n",
    "\n",
    "\"Train a classification model on the @customer_data dataset to predict churn. Evaluate the model with accuracy and AUC metrics.\"\n",
    "\n",
    "\"Perform hyperparameter tuning on a regression model using the @housing_prices dataset to improve prediction error.\"\n",
    "\n",
    "\"Build a clustering model on the @sales_leads dataset to identify customer segments and provide a summary of each cluster’s characteristics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdc3794-073f-4e4f-a333-cd91744feaa1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load UK Housing Dataset"
    }
   },
   "outputs": [],
   "source": [
    "# Load the UK housing dataset\n",
    "df = spark.table(\"caio_moreno.uk_house_advisor_bot.silver_pp_complete\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.count()} rows, {len(df.columns)} columns\")\n",
    "print(\"\\nColumn names and types:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14517ce-79d4-498c-b78d-cc51c26acc8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple Data Quality Check"
    }
   },
   "outputs": [],
   "source": [
    "# Simple data exploration\n",
    "from pyspark.sql.functions import col, count, when, min, max, avg, stddev\n",
    "\n",
    "# Basic counts\n",
    "total_rows = df.count()\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "\n",
    "# Check for nulls in key columns\n",
    "print(\"\\nNull counts in key columns:\")\n",
    "key_columns = ['price', 'date_of_transfer', 'postcode', 'property_type', 'county']\n",
    "for col_name in key_columns:\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    print(f\"{col_name}: {null_count} nulls ({null_count/total_rows*100:.2f}%)\")\n",
    "\n",
    "# Price statistics\n",
    "print(\"\\nPrice statistics:\")\n",
    "price_stats = df.select(\n",
    "    min('price').alias('min_price'),\n",
    "    max('price').alias('max_price'),\n",
    "    avg('price').alias('avg_price'),\n",
    "    stddev('price').alias('std_price')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Min price: £{price_stats['min_price']:,.2f}\")\n",
    "print(f\"Max price: £{price_stats['max_price']:,.2f}\")\n",
    "print(f\"Avg price: £{price_stats['avg_price']:,.2f}\")\n",
    "print(f\"Std price: £{price_stats['std_price']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef068735-6b45-44a1-a192-2b9d566a56e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Quality Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "from pyspark.sql.functions import col, when, isnan, isnull, count, sum as spark_sum\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"=== DUPLICATE ANALYSIS ===\")\n",
    "total_records = df.count()\n",
    "unique_transactions = df.select('transaction_id').distinct().count()\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Unique transaction IDs: {unique_transactions:,}\")\n",
    "print(f\"Duplicate transactions: {total_records - unique_transactions:,}\")\n",
    "\n",
    "# Check for invalid prices\n",
    "print(\"\\n=== PRICE QUALITY CHECKS ===\")\n",
    "invalid_prices = df.filter((col('price') <= 0) | col('price').isNull()).count()\n",
    "print(f\"Invalid prices (<=0 or null): {invalid_prices:,}\")\n",
    "\n",
    "# Extreme price outliers (using IQR method)\n",
    "price_quartiles = df.select('price').approxQuantile('price', [0.25, 0.75], 0.01)\n",
    "if len(price_quartiles) == 2:\n",
    "    q1, q3 = price_quartiles\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = df.filter((col('price') < lower_bound) | (col('price') > upper_bound)).count()\n",
    "    print(f\"Price outliers (IQR method): {outliers:,} ({outliers/total_records*100:.2f}%)\")\n",
    "    print(f\"Price range (Q1-Q3): £{q1:,.0f} - £{q3:,.0f}\")\n",
    "\n",
    "# Check date range\n",
    "print(\"\\n=== DATE QUALITY CHECKS ===\")\n",
    "date_stats = df.select(\n",
    "    min('date_of_transfer').alias('min_date'),\n",
    "    max('date_of_transfer').alias('max_date')\n",
    ").collect()[0]\n",
    "print(f\"Date range: {date_stats['min_date']} to {date_stats['max_date']}\")\n",
    "\n",
    "# Check for missing postcodes (critical for location features)\n",
    "missing_postcodes = df.filter(col('postcode').isNull() | (col('postcode') == '')).count()\n",
    "print(f\"Missing postcodes: {missing_postcodes:,} ({missing_postcodes/total_records*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7558bf72-0a03-4d40-818d-2a5e480a3feb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering - Date and Time Features"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering - Date and Time Features\n",
    "from pyspark.sql.functions import (\n",
    "    year, month, dayofmonth, dayofweek, quarter,\n",
    "    when, col, regexp_extract, length, split,\n",
    "    log, sqrt, abs as spark_abs\n",
    ")\n",
    "\n",
    "# Create date-based features\n",
    "df_features = df.withColumn('year', year('date_of_transfer')) \\\n",
    "                .withColumn('month', month('date_of_transfer')) \\\n",
    "                .withColumn('quarter', quarter('date_of_transfer')) \\\n",
    "                .withColumn('day_of_week', dayofweek('date_of_transfer')) \\\n",
    "                .withColumn('day_of_month', dayofmonth('date_of_transfer'))\n",
    "\n",
    "# Create seasonal features\n",
    "df_features = df_features.withColumn('season',\n",
    "    when(col('month').isin([12, 1, 2]), 'Winter')\n",
    "    .when(col('month').isin([3, 4, 5]), 'Spring')\n",
    "    .when(col('month').isin([6, 7, 8]), 'Summer')\n",
    "    .otherwise('Autumn')\n",
    ")\n",
    "\n",
    "# Weekend indicator\n",
    "df_features = df_features.withColumn('is_weekend',\n",
    "    when(col('day_of_week').isin([1, 7]), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Date and time features created:\")\n",
    "print(\"- year, month, quarter, day_of_week, day_of_month\")\n",
    "print(\"- season (Winter/Spring/Summer/Autumn)\")\n",
    "print(\"- is_weekend (0/1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b1c8c1-bf31-4920-8178-af3da0e6b2c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering - Price Features"
    }
   },
   "outputs": [],
   "source": [
    "# Price-based features\n",
    "from pyspark.sql.functions import log, when, col\n",
    "\n",
    "# Log price (helps with skewed price distribution)\n",
    "df_features = df_features.withColumn('log_price', log(col('price')))\n",
    "\n",
    "# Price categories based on UK housing market segments\n",
    "df_features = df_features.withColumn('price_category',\n",
    "    when(col('price') < 150000, 'Budget')\n",
    "    .when(col('price') < 300000, 'Mid-range')\n",
    "    .when(col('price') < 500000, 'Premium')\n",
    "    .when(col('price') < 1000000, 'Luxury')\n",
    "    .otherwise('Ultra-luxury')\n",
    ")\n",
    "\n",
    "# Price per square meter proxy (using property type as rough size indicator)\n",
    "# This is a simplified approach - in real scenarios you'd have actual size data\n",
    "df_features = df_features.withColumn('price_type_ratio',\n",
    "    when(col('property_type') == 'F', col('price') / 50)  # Flat - assume ~50 sqm avg\n",
    "    .when(col('property_type') == 'T', col('price') / 80)  # Terraced - assume ~80 sqm avg\n",
    "    .when(col('property_type') == 'S', col('price') / 90)  # Semi-detached - assume ~90 sqm avg\n",
    "    .when(col('property_type') == 'D', col('price') / 120) # Detached - assume ~120 sqm avg\n",
    "    .otherwise(col('price') / 75)  # Other - assume ~75 sqm avg\n",
    ")\n",
    "\n",
    "print(\"Price features created:\")\n",
    "print(\"- log_price (log transformation)\")\n",
    "print(\"- price_category (Budget/Mid-range/Premium/Luxury/Ultra-luxury)\")\n",
    "print(\"- price_type_ratio (price per estimated sqm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c37ea9-ced5-400e-b14a-bb8ad48a9a5d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering - Location Features"
    }
   },
   "outputs": [],
   "source": [
    "# Location-based features\n",
    "from pyspark.sql.functions import regexp_extract, length, split, col, when, upper\n",
    "\n",
    "# Extract postcode components\n",
    "# UK postcodes have format: Area(1-2 letters) + District(1-2 digits) + Sector(1 digit) + Unit(2 letters)\n",
    "df_features = df_features.withColumn('postcode_area', \n",
    "    regexp_extract(col('postcode'), r'^([A-Z]{1,2})', 1)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn('postcode_district',\n",
    "    regexp_extract(col('postcode'), r'^[A-Z]{1,2}([0-9]{1,2})', 1)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn('postcode_sector',\n",
    "    regexp_extract(col('postcode'), r'([0-9])[A-Z]{2}$', 1)\n",
    ")\n",
    "\n",
    "# London indicator (major price driver)\n",
    "london_areas = ['E', 'EC', 'N', 'NW', 'SE', 'SW', 'W', 'WC']\n",
    "df_features = df_features.withColumn('is_london',\n",
    "    when(col('postcode_area').isin(london_areas), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Major city indicators\n",
    "major_cities = ['Manchester', 'Birmingham', 'Leeds', 'Glasgow', 'Sheffield', \n",
    "                'Bradford', 'Liverpool', 'Edinburgh', 'Bristol', 'Cardiff']\n",
    "df_features = df_features.withColumn('is_major_city',\n",
    "    when(col('town_city').isin(major_cities), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# County grouping (group smaller counties)\n",
    "df_features = df_features.withColumn('county_group',\n",
    "    when(col('county').isin(['GREATER LONDON', 'LONDON']), 'London')\n",
    "    .when(col('county').isin(['GREATER MANCHESTER', 'MANCHESTER']), 'Manchester')\n",
    "    .when(col('county').isin(['WEST MIDLANDS', 'BIRMINGHAM']), 'West Midlands')\n",
    "    .when(col('county').isin(['WEST YORKSHIRE', 'SOUTH YORKSHIRE', 'NORTH YORKSHIRE']), 'Yorkshire')\n",
    "    .otherwise('Other')\n",
    ")\n",
    "\n",
    "print(\"Location features created:\")\n",
    "print(\"- postcode_area, postcode_district, postcode_sector\")\n",
    "print(\"- is_london (0/1)\")\n",
    "print(\"- is_major_city (0/1)\")\n",
    "print(\"- county_group (London/Manchester/West Midlands/Yorkshire/Other)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4967c0d-5629-4c40-b308-5012edf6ecd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering - Property Features"
    }
   },
   "outputs": [],
   "source": [
    "# Property-specific features\n",
    "from pyspark.sql.functions import col, when, length, regexp_extract\n",
    "\n",
    "# Property age indicator (new vs old)\n",
    "df_features = df_features.withColumn('is_new_property',\n",
    "    when(col('old_new') == 'Y', 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Leasehold vs Freehold\n",
    "df_features = df_features.withColumn('is_freehold',\n",
    "    when(col('duration') == 'F', 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Property type hierarchy (size/value proxy)\n",
    "df_features = df_features.withColumn('property_size_score',\n",
    "    when(col('property_type') == 'F', 1)  # Flat - smallest\n",
    "    .when(col('property_type') == 'T', 2)  # Terraced\n",
    "    .when(col('property_type') == 'S', 3)  # Semi-detached\n",
    "    .when(col('property_type') == 'D', 4)  # Detached - largest\n",
    "    .otherwise(2)  # Other - assume medium\n",
    ")\n",
    "\n",
    "# Address completeness score (more complete address might indicate better data quality/area)\n",
    "df_features = df_features.withColumn('address_completeness',\n",
    "    (when(col('paon').isNotNull() & (col('paon') != ''), 1).otherwise(0) +\n",
    "     when(col('saon').isNotNull() & (col('saon') != ''), 1).otherwise(0) +\n",
    "     when(col('street').isNotNull() & (col('street') != ''), 1).otherwise(0) +\n",
    "     when(col('locality').isNotNull() & (col('locality') != ''), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "print(\"Property features created:\")\n",
    "print(\"- is_new_property (0/1)\")\n",
    "print(\"- is_freehold (0/1)\")\n",
    "print(\"- property_size_score (1-4, size proxy)\")\n",
    "print(\"- address_completeness (0-4, completeness score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3faf9809-ad78-4594-9c5c-fcad3fc97c97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Categorical Variable Encoding"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare categorical variables for ML\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define categorical columns to encode\n",
    "categorical_cols = [\n",
    "    'property_type', 'county_group', 'season', 'price_category',\n",
    "    'postcode_area', 'ppd_category_type', 'record_status'\n",
    "]\n",
    "\n",
    "# Create StringIndexers for categorical variables\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + '_indexed', handleInvalid='keep') \n",
    "           for col in categorical_cols]\n",
    "\n",
    "# Create OneHotEncoders\n",
    "encoders = [OneHotEncoder(inputCol=col + '_indexed', outputCol=col + '_encoded') \n",
    "           for col in categorical_cols]\n",
    "\n",
    "# Combine indexers and encoders\n",
    "stages = indexers + encoders\n",
    "\n",
    "# Create and fit pipeline\n",
    "encoding_pipeline = Pipeline(stages=stages)\n",
    "encoding_model = encoding_pipeline.fit(df_features)\n",
    "df_encoded = encoding_model.transform(df_features)\n",
    "\n",
    "print(\"Categorical encoding completed for:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"- {col} -> {col}_indexed -> {col}_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "740daeed-754c-441d-8b07-d580d355068b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Cleaning and Filtering"
    }
   },
   "outputs": [],
   "source": [
    "# Data cleaning and filtering\n",
    "from pyspark.sql.functions import col, isnan, isnull\n",
    "\n",
    "# Remove records with invalid prices\n",
    "df_clean = df_encoded.filter(\n",
    "    (col('price') > 0) & \n",
    "    col('price').isNotNull() & \n",
    "    (col('price') < 10000000)  # Remove extreme outliers (>£10M)\n",
    ")\n",
    "\n",
    "# Remove records with missing critical information\n",
    "df_clean = df_clean.filter(\n",
    "    col('postcode').isNotNull() & \n",
    "    (col('postcode') != '') &\n",
    "    col('property_type').isNotNull() &\n",
    "    col('date_of_transfer').isNotNull()\n",
    ")\n",
    "\n",
    "# Remove duplicate transactions (keep first occurrence)\n",
    "df_clean = df_clean.dropDuplicates(['transaction_id'])\n",
    "\n",
    "print(f\"Data cleaning completed:\")\n",
    "original_count = df_encoded.count()\n",
    "cleaned_count = df_clean.count()\n",
    "print(f\"Original records: {original_count:,}\")\n",
    "print(f\"Cleaned records: {cleaned_count:,}\")\n",
    "print(f\"Removed: {original_count - cleaned_count:,} ({(original_count - cleaned_count)/original_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e72cda4-b875-4d53-ae56-4f173f8bc9f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Scaling and Selection"
    }
   },
   "outputs": [],
   "source": [
    "# Feature scaling and final dataset preparation\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define numerical features for scaling\n",
    "numerical_features = [\n",
    "    'log_price', 'year', 'month', 'quarter', 'day_of_week',\n",
    "    'property_size_score', 'address_completeness', 'price_type_ratio'\n",
    "]\n",
    "\n",
    "# Define binary features (no scaling needed)\n",
    "binary_features = [\n",
    "    'is_weekend', 'is_new_property', 'is_freehold', 'is_london', 'is_major_city'\n",
    "]\n",
    "\n",
    "# Define encoded categorical features\n",
    "encoded_features = [\n",
    "    'property_type_encoded', 'county_group_encoded', 'season_encoded',\n",
    "    'postcode_area_encoded', 'ppd_category_type_encoded'\n",
    "]\n",
    "\n",
    "# Assemble numerical features for scaling\n",
    "numerical_assembler = VectorAssembler(\n",
    "    inputCols=numerical_features,\n",
    "    outputCol='numerical_features_raw'\n",
    ")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler(\n",
    "    inputCol='numerical_features_raw',\n",
    "    outputCol='numerical_features_scaled',\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Create scaling pipeline\n",
    "scaling_pipeline = Pipeline(stages=[numerical_assembler, scaler])\n",
    "scaling_model = scaling_pipeline.fit(df_clean)\n",
    "df_scaled = scaling_model.transform(df_clean)\n",
    "\n",
    "print(\"Feature scaling completed for numerical features:\")\n",
    "for feat in numerical_features:\n",
    "    print(f\"- {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9349e526-a921-4fa6-9c50-74d7bf15ceeb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final ML Dataset Assembly"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble final feature vector for ML\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Combine all features into final feature vector\n",
    "all_feature_cols = (\n",
    "    binary_features + \n",
    "    encoded_features + \n",
    "    ['numerical_features_scaled']\n",
    ")\n",
    "\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=all_feature_cols,\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Create final ML-ready dataset\n",
    "df_ml_ready = final_assembler.transform(df_scaled)\n",
    "\n",
    "# Select only necessary columns for ML\n",
    "ml_dataset = df_ml_ready.select(\n",
    "    'transaction_id',\n",
    "    'price',  # target variable\n",
    "    'log_price',  # alternative target (for regression)\n",
    "    'features',  # feature vector\n",
    "    'date_of_transfer',\n",
    "    'postcode',\n",
    "    'property_type',\n",
    "    'county',\n",
    "    'town_city'\n",
    ")\n",
    "\n",
    "print(\"Final ML dataset created with columns:\")\n",
    "for col in ml_dataset.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {ml_dataset.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0abc4257-b77e-4755-8e89-5ee32aa60234",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Preparation Summary and Save"
    }
   },
   "outputs": [],
   "source": [
    "# Create train/validation/test splits\n",
    "from pyspark.sql.functions import rand, col\n",
    "\n",
    "# Add random column for splitting\n",
    "ml_dataset_with_split = ml_dataset.withColumn('rand', rand(seed=42))\n",
    "\n",
    "# Create splits: 70% train, 15% validation, 15% test\n",
    "train_data = ml_dataset_with_split.filter(col('rand') < 0.7).drop('rand')\n",
    "val_data = ml_dataset_with_split.filter((col('rand') >= 0.7) & (col('rand') < 0.85)).drop('rand')\n",
    "test_data = ml_dataset_with_split.filter(col('rand') >= 0.85).drop('rand')\n",
    "\n",
    "print(\"=== DATA PREPARATION SUMMARY ===\")\n",
    "print(f\"Training set: {train_data.count():,} records ({train_data.count()/ml_dataset.count()*100:.1f}%)\")\n",
    "print(f\"Validation set: {val_data.count():,} records ({val_data.count()/ml_dataset.count()*100:.1f}%)\")\n",
    "print(f\"Test set: {test_data.count():,} records ({test_data.count()/ml_dataset.count()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== FEATURES CREATED ===\")\n",
    "print(\"Date Features: year, month, quarter, season, day_of_week, is_weekend\")\n",
    "print(\"Price Features: log_price, price_category, price_type_ratio\")\n",
    "print(\"Location Features: postcode components, is_london, is_major_city, county_group\")\n",
    "print(\"Property Features: is_new_property, is_freehold, property_size_score, address_completeness\")\n",
    "print(\"Categorical Encoding: One-hot encoded for all categorical variables\")\n",
    "print(\"Numerical Scaling: StandardScaler applied to continuous features\")\n",
    "\n",
    "# Save the prepared datasets (optional)\n",
    "# Uncomment the lines below to save the datasets\n",
    "# train_data.write.mode('overwrite').saveAsTable('caio_moreno.uk_house_advisor_bot.ml_train_data')\n",
    "# val_data.write.mode('overwrite').saveAsTable('caio_moreno.uk_house_advisor_bot.ml_val_data')\n",
    "# test_data.write.mode('overwrite').saveAsTable('caio_moreno.uk_house_advisor_bot.ml_test_data')\n",
    "\n",
    "print(\"\\n✅ Data preparation completed! Ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0503f560-7426-483e-9259-c76fd18fe2a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Training Setup and Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Model Training Setup\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, sqrt, abs as spark_abs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set up evaluation metrics\n",
    "evaluator_rmse = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='rmse')\n",
    "evaluator_mae = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='mae')\n",
    "evaluator_r2 = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='r2')\n",
    "\n",
    "print(\"Model training setup completed!\")\n",
    "print(\"Available models: Linear Regression, Random Forest, Gradient Boosting Trees\")\n",
    "print(\"Evaluation metrics: RMSE, MAE, R²\")\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nTraining data: {train_data.count():,} records\")\n",
    "print(f\"Validation data: {val_data.count():,} records\")\n",
    "print(f\"Test data: {test_data.count():,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26206e63-dc9c-4935-be51-0446697cccec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train Baseline Models"
    }
   },
   "outputs": [],
   "source": [
    "# Train baseline models\n",
    "print(\"=== TRAINING BASELINE MODELS ===\")\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"\\n1. Training Linear Regression...\")\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price', maxIter=100)\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(val_data)\n",
    "\n",
    "# Evaluate Linear Regression\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
    "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Linear Regression - RMSE: £{lr_rmse:,.2f}, MAE: £{lr_mae:,.2f}, R²: {lr_r2:.4f}\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n2. Training Random Forest...\")\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='price', numTrees=50, maxDepth=10, seed=42)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(val_data)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
    "rf_mae = evaluator_mae.evaluate(rf_predictions)\n",
    "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest - RMSE: £{rf_rmse:,.2f}, MAE: £{rf_mae:,.2f}, R²: {rf_r2:.4f}\")\n",
    "\n",
    "# 3. Gradient Boosting Trees\n",
    "print(\"\\n3. Training Gradient Boosting Trees...\")\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='price', maxIter=50, maxDepth=8, seed=42)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "gbt_predictions = gbt_model.transform(val_data)\n",
    "\n",
    "# Evaluate GBT\n",
    "gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)\n",
    "gbt_mae = evaluator_mae.evaluate(gbt_predictions)\n",
    "gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"Gradient Boosting - RMSE: £{gbt_rmse:,.2f}, MAE: £{gbt_mae:,.2f}, R²: {gbt_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b49854-faeb-4653-87d7-5245a11ab36b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Performance Comparison"
    }
   },
   "outputs": [],
   "source": [
    "# Create performance comparison\n",
    "print(\"\\n=== MODEL PERFORMANCE COMPARISON ===\")\n",
    "\n",
    "# Store results in a dictionary\n",
    "results = {\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "    'RMSE': [lr_rmse, rf_rmse, gbt_rmse],\n",
    "    'MAE': [lr_mae, rf_mae, gbt_mae],\n",
    "    'R²': [lr_r2, rf_r2, gbt_r2]\n",
    "}\n",
    "\n",
    "# Convert to pandas for easier visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_df['RMSE'].idxmin()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\n✅ Best performing model: {best_model_name}\")\n",
    "print(f\"Best RMSE: £{results_df.loc[best_model_idx, 'RMSE']:,.2f}\")\n",
    "print(f\"Best R²: {results_df.loc[best_model_idx, 'R²']:.4f}\")\n",
    "\n",
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].bar(results_df['Model'], results_df['RMSE'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "axes[0].set_title('RMSE Comparison')\n",
    "axes[0].set_ylabel('RMSE (£)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(results_df['Model'], results_df['MAE'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "axes[1].set_title('MAE Comparison')\n",
    "axes[1].set_ylabel('MAE (£)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R² comparison\n",
    "axes[2].bar(results_df['Model'], results_df['R²'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "axes[2].set_title('R² Comparison')\n",
    "axes[2].set_ylabel('R² Score')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047348ba-4028-4261-901b-ead582a0ca6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model\n",
    "print(\"=== HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# We'll tune the Random Forest (typically performs well) and GBT\n",
    "# Start with Random Forest tuning\n",
    "print(\"\\nTuning Random Forest hyperparameters...\")\n",
    "\n",
    "# Create parameter grid for Random Forest\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(RandomForestRegressor.numTrees, [30, 50, 100]) \\\n",
    "    .addGrid(RandomForestRegressor.maxDepth, [8, 12, 16]) \\\n",
    "    .addGrid(RandomForestRegressor.minInstancesPerNode, [1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "# Create CrossValidator for Random Forest\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=RandomForestRegressor(featuresCol='features', labelCol='price', seed=42),\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=evaluator_rmse,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Fit the cross validator\n",
    "rf_cv_model = rf_cv.fit(train_data)\n",
    "rf_tuned_predictions = rf_cv_model.transform(val_data)\n",
    "\n",
    "# Evaluate tuned Random Forest\n",
    "rf_tuned_rmse = evaluator_rmse.evaluate(rf_tuned_predictions)\n",
    "rf_tuned_mae = evaluator_mae.evaluate(rf_tuned_predictions)\n",
    "rf_tuned_r2 = evaluator_r2.evaluate(rf_tuned_predictions)\n",
    "\n",
    "print(f\"Tuned Random Forest - RMSE: £{rf_tuned_rmse:,.2f}, MAE: £{rf_tuned_mae:,.2f}, R²: {rf_tuned_r2:.4f}\")\n",
    "\n",
    "# Get best parameters\n",
    "best_rf_model = rf_cv_model.bestModel\n",
    "print(f\"Best RF parameters: numTrees={best_rf_model.getNumTrees}, maxDepth={best_rf_model.getMaxDepth}, minInstancesPerNode={best_rf_model.getMinInstancesPerNode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f8b9c3-dd1b-4f94-820c-6c7387d3b296",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GBT Hyperparameter Tuning"
    }
   },
   "outputs": [],
   "source": [
    "# Tune Gradient Boosting Trees\n",
    "print(\"\\nTuning Gradient Boosting Trees hyperparameters...\")\n",
    "\n",
    "# Create parameter grid for GBT\n",
    "gbt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(GBTRegressor.maxIter, [30, 50, 100]) \\\n",
    "    .addGrid(GBTRegressor.maxDepth, [6, 8, 10]) \\\n",
    "    .addGrid(GBTRegressor.stepSize, [0.1, 0.2]) \\\n",
    "    .build()\n",
    "\n",
    "# Create CrossValidator for GBT\n",
    "gbt_cv = CrossValidator(\n",
    "    estimator=GBTRegressor(featuresCol='features', labelCol='price', seed=42),\n",
    "    estimatorParamMaps=gbt_param_grid,\n",
    "    evaluator=evaluator_rmse,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Fit the cross validator\n",
    "gbt_cv_model = gbt_cv.fit(train_data)\n",
    "gbt_tuned_predictions = gbt_cv_model.transform(val_data)\n",
    "\n",
    "# Evaluate tuned GBT\n",
    "gbt_tuned_rmse = evaluator_rmse.evaluate(gbt_tuned_predictions)\n",
    "gbt_tuned_mae = evaluator_mae.evaluate(gbt_tuned_predictions)\n",
    "gbt_tuned_r2 = evaluator_r2.evaluate(gbt_tuned_predictions)\n",
    "\n",
    "print(f\"Tuned GBT - RMSE: £{gbt_tuned_rmse:,.2f}, MAE: £{gbt_tuned_mae:,.2f}, R²: {gbt_tuned_r2:.4f}\")\n",
    "\n",
    "# Get best parameters\n",
    "best_gbt_model = gbt_cv_model.bestModel\n",
    "print(f\"Best GBT parameters: maxIter={best_gbt_model.getMaxIter}, maxDepth={best_gbt_model.getMaxDepth}, stepSize={best_gbt_model.getStepSize}\")\n",
    "\n",
    "# Compare tuned models\n",
    "print(\"\\n=== TUNED MODEL COMPARISON ===\")\n",
    "tuned_results = {\n",
    "    'Model': ['Tuned Random Forest', 'Tuned GBT'],\n",
    "    'RMSE': [rf_tuned_rmse, gbt_tuned_rmse],\n",
    "    'MAE': [rf_tuned_mae, gbt_tuned_mae],\n",
    "    'R²': [rf_tuned_r2, gbt_tuned_r2]\n",
    "}\n",
    "\n",
    "tuned_df = pd.DataFrame(tuned_results)\n",
    "print(tuned_df.to_string(index=False))\n",
    "\n",
    "# Select best tuned model\n",
    "best_tuned_idx = tuned_df['RMSE'].idxmin()\n",
    "best_tuned_name = tuned_df.loc[best_tuned_idx, 'Model']\n",
    "print(f\"\\n✅ Best tuned model: {best_tuned_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1da20c-3eaa-48af-9d6d-903a772ca1c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final Model Validation on Test Set"
    }
   },
   "outputs": [],
   "source": [
    "# Final model validation on test set\n",
    "print(\"=== FINAL MODEL VALIDATION ===\")\n",
    "\n",
    "# Select the best model (assume it's the tuned Random Forest for now)\n",
    "# You can modify this based on the actual results\n",
    "if rf_tuned_rmse < gbt_tuned_rmse:\n",
    "    final_model = rf_cv_model.bestModel\n",
    "    final_model_name = \"Tuned Random Forest\"\n",
    "    final_predictions = rf_cv_model.transform(test_data)\n",
    "else:\n",
    "    final_model = gbt_cv_model.bestModel\n",
    "    final_model_name = \"Tuned GBT\"\n",
    "    final_predictions = gbt_cv_model.transform(test_data)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_rmse = evaluator_rmse.evaluate(final_predictions)\n",
    "test_mae = evaluator_mae.evaluate(final_predictions)\n",
    "test_r2 = evaluator_r2.evaluate(final_predictions)\n",
    "\n",
    "print(f\"Final Model: {final_model_name}\")\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  RMSE: £{test_rmse:,.2f}\")\n",
    "print(f\"  MAE: £{test_mae:,.2f}\")\n",
    "print(f\"  R²: {test_r2:.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_predictions_pd = final_predictions.select('price', 'prediction').toPandas()\n",
    "mape = np.mean(np.abs((test_predictions_pd['price'] - test_predictions_pd['prediction']) / test_predictions_pd['price'])) * 100\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Prediction vs Actual scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(test_predictions_pd['price'], test_predictions_pd['prediction'], alpha=0.5)\n",
    "plt.plot([test_predictions_pd['price'].min(), test_predictions_pd['price'].max()], \n",
    "         [test_predictions_pd['price'].min(), test_predictions_pd['price'].max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price (£)')\n",
    "plt.ylabel('Predicted Price (£)')\n",
    "plt.title(f'{final_model_name} - Predictions vs Actual')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Model validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622e915f-026e-4154-b01f-f18cf73b808e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Importance Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Feature importance analysis (for tree-based models)\n",
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "if hasattr(final_model, 'featureImportances'):\n",
    "    # Get feature importances\n",
    "    importances = final_model.featureImportances.toArray()\n",
    "    \n",
    "    # Create feature names (this is a simplified version)\n",
    "    # In practice, you'd need to map back to original feature names\n",
    "    feature_names = [\n",
    "        'is_weekend', 'is_new_property', 'is_freehold', 'is_london', 'is_major_city',\n",
    "        'property_type_encoded', 'county_group_encoded', 'season_encoded',\n",
    "        'postcode_area_encoded', 'ppd_category_type_encoded', 'numerical_features'\n",
    "    ]\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names[:len(importances)],\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'{final_model_name} - Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance not available for this model type.\")\n",
    "\n",
    "print(\"\\n=== MODEL TRAINING SUMMARY ===\")\n",
    "print(f\"Final Model: {final_model_name}\")\n",
    "print(f\"Training Data: {train_data.count():,} records\")\n",
    "print(f\"Validation Data: {val_data.count():,} records\")\n",
    "print(f\"Test Data: {test_data.count():,} records\")\n",
    "print(f\"Final Test RMSE: £{test_rmse:,.2f}\")\n",
    "print(f\"Final Test R²: {test_r2:.4f}\")\n",
    "print(\"\\n✅ House price prediction model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b74cd2-d4e4-4dc5-bb09-7aa17a9d08e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fe22aa-0de3-46f4-bb70-d9419df666bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ml_dataset_with_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db1c169-fa83-4677-9c58-367bf680c4b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027b77ba-2c77-405f-962f-394decd594a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f12b68-8842-4431-acd5-f72124980ac5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dataset Statistics and Exploration"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset statistics\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "\n",
    "# Check data types and null counts\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "# Calculate null counts for each column (handle different data types)\n",
    "null_counts = []\n",
    "for c in df.columns:\n",
    "    if isinstance(df.schema[c].dataType, (DoubleType, FloatType)):\n",
    "        null_count = df.select(count(when(col(c).isNull() | isnan(col(c)), c)).alias(c))\n",
    "    else:\n",
    "        null_count = df.select(count(when(col(c).isNull(), c)).alias(c))\n",
    "    null_counts.append(null_count)\n",
    "\n",
    "# Combine all null counts\n",
    "from functools import reduce\n",
    "null_summary = reduce(lambda df1, df2: df1.union(df2), null_counts)\n",
    "print(\"\\nNull counts by column:\")\n",
    "null_summary.show()\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"\\nBasic statistics for price column:\")\n",
    "df.select(\"price\").describe().show()\n",
    "\n",
    "# Check unique values for categorical columns\n",
    "print(\"\\nUnique values in key categorical columns:\")\n",
    "print(f\"Property types: {df.select('property_type').distinct().count()}\")\n",
    "print(f\"Counties: {df.select('county').distinct().count()}\")\n",
    "print(f\"Towns/Cities: {df.select('town_city').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7565876f-4a25-4fd2-bb7f-912e843dec88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1.Train_Basic_ML_Model_DB_DS_Agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a2a43d4-5ff3-43e9-ab14-640cc78e6af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use the Data Science Agent\n",
    "https://docs.databricks.com/aws/en/notebooks/ds-agent\n",
    "\n",
    "Machine learning:\n",
    "\"Perform some data preparation and feature engineering to prepare this dataset for model training.\"\n",
    "\n",
    "\"Train a classification model on the @customer_data dataset to predict churn. Evaluate the model with accuracy and AUC metrics.\"\n",
    "\n",
    "\"Perform hyperparameter tuning on a regression model using the @housing_prices dataset to improve prediction error.\"\n",
    "\n",
    "\"Build a clustering model on the @sales_leads dataset to identify customer segments and provide a summary of each cluster’s characteristics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdc3794-073f-4e4f-a333-cd91744feaa1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load UK Housing Dataset"
    }
   },
   "outputs": [],
   "source": [
    "# Load the UK housing dataset\n",
    "df = spark.table(\"caio_moreno.uk_house_advisor_bot.silver_pp_complete\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.count()} rows, {len(df.columns)} columns\")\n",
    "print(\"\\nColumn names and types:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14517ce-79d4-498c-b78d-cc51c26acc8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple Data Quality Check"
    }
   },
   "outputs": [],
   "source": [
    "# Simple data exploration\n",
    "from pyspark.sql.functions import col, count, when, min, max, avg, stddev\n",
    "\n",
    "# Basic counts\n",
    "total_rows = df.count()\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "\n",
    "# Check for nulls in key columns\n",
    "print(\"\\nNull counts in key columns:\")\n",
    "key_columns = ['price', 'date_of_transfer', 'postcode', 'property_type', 'county']\n",
    "for col_name in key_columns:\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    print(f\"{col_name}: {null_count} nulls ({null_count/total_rows*100:.2f}%)\")\n",
    "\n",
    "# Price statistics\n",
    "print(\"\\nPrice statistics:\")\n",
    "price_stats = df.select(\n",
    "    min('price').alias('min_price'),\n",
    "    max('price').alias('max_price'),\n",
    "    avg('price').alias('avg_price'),\n",
    "    stddev('price').alias('std_price')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Min price: £{price_stats['min_price']:,.2f}\")\n",
    "print(f\"Max price: £{price_stats['max_price']:,.2f}\")\n",
    "print(f\"Avg price: £{price_stats['avg_price']:,.2f}\")\n",
    "print(f\"Std price: £{price_stats['std_price']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef068735-6b45-44a1-a192-2b9d566a56e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Quality Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "from pyspark.sql.functions import col, when, isnan, isnull, count, sum as spark_sum\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"=== DUPLICATE ANALYSIS ===\")\n",
    "total_records = df.count()\n",
    "unique_transactions = df.select('transaction_id').distinct().count()\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Unique transaction IDs: {unique_transactions:,}\")\n",
    "print(f\"Duplicate transactions: {total_records - unique_transactions:,}\")\n",
    "\n",
    "# Check for invalid prices\n",
    "print(\"\\n=== PRICE QUALITY CHECKS ===\")\n",
    "invalid_prices = df.filter((col('price') <= 0) | col('price').isNull()).count()\n",
    "print(f\"Invalid prices (<=0 or null): {invalid_prices:,}\")\n",
    "\n",
    "# Extreme price outliers (using IQR method)\n",
    "price_quartiles = df.select('price').approxQuantile('price', [0.25, 0.75], 0.01)\n",
    "if len(price_quartiles) == 2:\n",
    "    q1, q3 = price_quartiles\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = df.filter((col('price') < lower_bound) | (col('price') > upper_bound)).count()\n",
    "    print(f\"Price outliers (IQR method): {outliers:,} ({outliers/total_records*100:.2f}%)\")\n",
    "    print(f\"Price range (Q1-Q3): £{q1:,.0f} - £{q3:,.0f}\")\n",
    "\n",
    "# Check date range\n",
    "print(\"\\n=== DATE QUALITY CHECKS ===\")\n",
    "date_stats = df.select(\n",
    "    min('date_of_transfer').alias('min_date'),\n",
    "    max('date_of_transfer').alias('max_date')\n",
    ").collect()[0]\n",
    "print(f\"Date range: {date_stats['min_date']} to {date_stats['max_date']}\")\n",
    "\n",
    "# Check for missing postcodes (critical for location features)\n",
    "missing_postcodes = df.filter(col('postcode').isNull() | (col('postcode') == '')).count()\n",
    "print(f\"Missing postcodes: {missing_postcodes:,} ({missing_postcodes/total_records*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7558bf72-0a03-4d40-818d-2a5e480a3feb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering - Date and Time Features"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering - Date and Time Features\n",
    "from pyspark.sql.functions import (\n",
    "    year, month, dayofmonth, dayofweek, quarter,\n",
    "    when, col, regexp_extract, length, split,\n",
    "    log, sqrt, abs as spark_abs\n",
    ")\n",
    "\n",
    "# Create date-based features\n",
    "df_features = df.withColumn('year', year('date_of_transfer')) \\\n",
    "                .withColumn('month', month('date_of_transfer')) \\\n",
    "                .withColumn('quarter', quarter('date_of_transfer')) \\\n",
    "                .withColumn('day_of_week', dayofweek('date_of_transfer')) \\\n",
    "                .withColumn('day_of_month', dayofmonth('date_of_transfer'))\n",
    "\n",
    "# Create seasonal features\n",
    "df_features = df_features.withColumn('season',\n",
    "    when(col('month').isin([12, 1, 2]), 'Winter')\n",
    "    .when(col('month').isin([3, 4, 5]), 'Spring')\n",
    "    .when(col('month').isin([6, 7, 8]), 'Summer')\n",
    "    .otherwise('Autumn')\n",
    ")\n",
    "\n",
    "# Weekend indicator\n",
    "df_features = df_features.withColumn('is_weekend',\n",
    "    when(col('day_of_week').isin([1, 7]), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Date and time features created:\")\n",
    "print(\"- year, month, quarter, day_of_week, day_of_month\")\n",
    "print(\"- season (Winter/Spring/Summer/Autumn)\")\n",
    "print(\"- is_weekend (0/1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b1c8c1-bf31-4920-8178-af3da0e6b2c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering - Price Features"
    }
   },
   "outputs": [],
   "source": [
    "# Price-based features\n",
    "from pyspark.sql.functions import log, when, col\n",
    "\n",
    "# Log price (helps with skewed price distribution)\n",
    "df_features = df_features.withColumn('log_price', log(col('price')))\n",
    "\n",
    "# Price categories based on UK housing market segments\n",
    "df_features = df_features.withColumn('price_category',\n",
    "    when(col('price') < 150000, 'Budget')\n",
    "    .when(col('price') < 300000, 'Mid-range')\n",
    "    .when(col('price') < 500000, 'Premium')\n",
    "    .when(col('price') < 1000000, 'Luxury')\n",
    "    .otherwise('Ultra-luxury')\n",
    ")\n",
    "\n",
    "# Price per square meter proxy (using property type as rough size indicator)\n",
    "# This is a simplified approach - in real scenarios you'd have actual size data\n",
    "df_features = df_features.withColumn('price_type_ratio',\n",
    "    when(col('property_type') == 'F', col('price') / 50)  # Flat - assume ~50 sqm avg\n",
    "    .when(col('property_type') == 'T', col('price') / 80)  # Terraced - assume ~80 sqm avg\n",
    "    .when(col('property_type') == 'S', col('price') / 90)  # Semi-detached - assume ~90 sqm avg\n",
    "    .when(col('property_type') == 'D', col('price') / 120) # Detached - assume ~120 sqm avg\n",
    "    .otherwise(col('price') / 75)  # Other - assume ~75 sqm avg\n",
    ")\n",
    "\n",
    "print(\"Price features created:\")\n",
    "print(\"- log_price (log transformation)\")\n",
    "print(\"- price_category (Budget/Mid-range/Premium/Luxury/Ultra-luxury)\")\n",
    "print(\"- price_type_ratio (price per estimated sqm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c37ea9-ced5-400e-b14a-bb8ad48a9a5d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering - Location Features"
    }
   },
   "outputs": [],
   "source": [
    "# Location-based features\n",
    "from pyspark.sql.functions import regexp_extract, length, split, col, when, upper\n",
    "\n",
    "# Extract postcode components\n",
    "# UK postcodes have format: Area(1-2 letters) + District(1-2 digits) + Sector(1 digit) + Unit(2 letters)\n",
    "df_features = df_features.withColumn('postcode_area', \n",
    "    regexp_extract(col('postcode'), r'^([A-Z]{1,2})', 1)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn('postcode_district',\n",
    "    regexp_extract(col('postcode'), r'^[A-Z]{1,2}([0-9]{1,2})', 1)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn('postcode_sector',\n",
    "    regexp_extract(col('postcode'), r'([0-9])[A-Z]{2}$', 1)\n",
    ")\n",
    "\n",
    "# London indicator (major price driver)\n",
    "london_areas = ['E', 'EC', 'N', 'NW', 'SE', 'SW', 'W', 'WC']\n",
    "df_features = df_features.withColumn('is_london',\n",
    "    when(col('postcode_area').isin(london_areas), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Major city indicators\n",
    "major_cities = ['Manchester', 'Birmingham', 'Leeds', 'Glasgow', 'Sheffield', \n",
    "                'Bradford', 'Liverpool', 'Edinburgh', 'Bristol', 'Cardiff']\n",
    "df_features = df_features.withColumn('is_major_city',\n",
    "    when(col('town_city').isin(major_cities), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# County grouping (group smaller counties)\n",
    "df_features = df_features.withColumn('county_group',\n",
    "    when(col('county').isin(['GREATER LONDON', 'LONDON']), 'London')\n",
    "    .when(col('county').isin(['GREATER MANCHESTER', 'MANCHESTER']), 'Manchester')\n",
    "    .when(col('county').isin(['WEST MIDLANDS', 'BIRMINGHAM']), 'West Midlands')\n",
    "    .when(col('county').isin(['WEST YORKSHIRE', 'SOUTH YORKSHIRE', 'NORTH YORKSHIRE']), 'Yorkshire')\n",
    "    .otherwise('Other')\n",
    ")\n",
    "\n",
    "print(\"Location features created:\")\n",
    "print(\"- postcode_area, postcode_district, postcode_sector\")\n",
    "print(\"- is_london (0/1)\")\n",
    "print(\"- is_major_city (0/1)\")\n",
    "print(\"- county_group (London/Manchester/West Midlands/Yorkshire/Other)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4967c0d-5629-4c40-b308-5012edf6ecd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering - Property Features"
    }
   },
   "outputs": [],
   "source": [
    "# Property-specific features\n",
    "from pyspark.sql.functions import col, when, length, regexp_extract\n",
    "\n",
    "# Property age indicator (new vs old)\n",
    "df_features = df_features.withColumn('is_new_property',\n",
    "    when(col('old_new') == 'Y', 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Leasehold vs Freehold\n",
    "df_features = df_features.withColumn('is_freehold',\n",
    "    when(col('duration') == 'F', 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Property type hierarchy (size/value proxy)\n",
    "df_features = df_features.withColumn('property_size_score',\n",
    "    when(col('property_type') == 'F', 1)  # Flat - smallest\n",
    "    .when(col('property_type') == 'T', 2)  # Terraced\n",
    "    .when(col('property_type') == 'S', 3)  # Semi-detached\n",
    "    .when(col('property_type') == 'D', 4)  # Detached - largest\n",
    "    .otherwise(2)  # Other - assume medium\n",
    ")\n",
    "\n",
    "# Address completeness score (more complete address might indicate better data quality/area)\n",
    "df_features = df_features.withColumn('address_completeness',\n",
    "    (when(col('paon').isNotNull() & (col('paon') != ''), 1).otherwise(0) +\n",
    "     when(col('saon').isNotNull() & (col('saon') != ''), 1).otherwise(0) +\n",
    "     when(col('street').isNotNull() & (col('street') != ''), 1).otherwise(0) +\n",
    "     when(col('locality').isNotNull() & (col('locality') != ''), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "print(\"Property features created:\")\n",
    "print(\"- is_new_property (0/1)\")\n",
    "print(\"- is_freehold (0/1)\")\n",
    "print(\"- property_size_score (1-4, size proxy)\")\n",
    "print(\"- address_completeness (0-4, completeness score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3faf9809-ad78-4594-9c5c-fcad3fc97c97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Categorical Variable Encoding"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare categorical variables for ML\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define categorical columns to encode\n",
    "categorical_cols = [\n",
    "    'property_type', 'county_group', 'season', 'price_category',\n",
    "    'postcode_area', 'ppd_category_type', 'record_status'\n",
    "]\n",
    "\n",
    "# Create StringIndexers for categorical variables\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + '_indexed', handleInvalid='keep') \n",
    "           for col in categorical_cols]\n",
    "\n",
    "# Create OneHotEncoders\n",
    "encoders = [OneHotEncoder(inputCol=col + '_indexed', outputCol=col + '_encoded') \n",
    "           for col in categorical_cols]\n",
    "\n",
    "# Combine indexers and encoders\n",
    "stages = indexers + encoders\n",
    "\n",
    "# Create and fit pipeline\n",
    "encoding_pipeline = Pipeline(stages=stages)\n",
    "encoding_model = encoding_pipeline.fit(df_features)\n",
    "df_encoded = encoding_model.transform(df_features)\n",
    "\n",
    "print(\"Categorical encoding completed for:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"- {col} -> {col}_indexed -> {col}_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "740daeed-754c-441d-8b07-d580d355068b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Cleaning and Filtering"
    }
   },
   "outputs": [],
   "source": [
    "# Data cleaning and filtering\n",
    "from pyspark.sql.functions import col, isnan, isnull\n",
    "\n",
    "# Remove records with invalid prices\n",
    "df_clean = df_encoded.filter(\n",
    "    (col('price') > 0) & \n",
    "    col('price').isNotNull() & \n",
    "    (col('price') < 10000000)  # Remove extreme outliers (>£10M)\n",
    ")\n",
    "\n",
    "# Remove records with missing critical information\n",
    "df_clean = df_clean.filter(\n",
    "    col('postcode').isNotNull() & \n",
    "    (col('postcode') != '') &\n",
    "    col('property_type').isNotNull() &\n",
    "    col('date_of_transfer').isNotNull()\n",
    ")\n",
    "\n",
    "# Remove duplicate transactions (keep first occurrence)\n",
    "df_clean = df_clean.dropDuplicates(['transaction_id'])\n",
    "\n",
    "print(f\"Data cleaning completed:\")\n",
    "original_count = df_encoded.count()\n",
    "cleaned_count = df_clean.count()\n",
    "print(f\"Original records: {original_count:,}\")\n",
    "print(f\"Cleaned records: {cleaned_count:,}\")\n",
    "print(f\"Removed: {original_count - cleaned_count:,} ({(original_count - cleaned_count)/original_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e72cda4-b875-4d53-ae56-4f173f8bc9f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Scaling and Selection"
    }
   },
   "outputs": [],
   "source": [
    "# Feature scaling and final dataset preparation\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define numerical features for scaling\n",
    "numerical_features = [\n",
    "    'log_price', 'year', 'month', 'quarter', 'day_of_week',\n",
    "    'property_size_score', 'address_completeness', 'price_type_ratio'\n",
    "]\n",
    "\n",
    "# Define binary features (no scaling needed)\n",
    "binary_features = [\n",
    "    'is_weekend', 'is_new_property', 'is_freehold', 'is_london', 'is_major_city'\n",
    "]\n",
    "\n",
    "# Define encoded categorical features\n",
    "encoded_features = [\n",
    "    'property_type_encoded', 'county_group_encoded', 'season_encoded',\n",
    "    'postcode_area_encoded', 'ppd_category_type_encoded'\n",
    "]\n",
    "\n",
    "# Assemble numerical features for scaling\n",
    "numerical_assembler = VectorAssembler(\n",
    "    inputCols=numerical_features,\n",
    "    outputCol='numerical_features_raw'\n",
    ")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler(\n",
    "    inputCol='numerical_features_raw',\n",
    "    outputCol='numerical_features_scaled',\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Create scaling pipeline\n",
    "scaling_pipeline = Pipeline(stages=[numerical_assembler, scaler])\n",
    "scaling_model = scaling_pipeline.fit(df_clean)\n",
    "df_scaled = scaling_model.transform(df_clean)\n",
    "\n",
    "print(\"Feature scaling completed for numerical features:\")\n",
    "for feat in numerical_features:\n",
    "    print(f\"- {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9349e526-a921-4fa6-9c50-74d7bf15ceeb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final ML Dataset Assembly"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble final feature vector for ML\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Combine all features into final feature vector\n",
    "all_feature_cols = (\n",
    "    binary_features + \n",
    "    encoded_features + \n",
    "    ['numerical_features_scaled']\n",
    ")\n",
    "\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=all_feature_cols,\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Create final ML-ready dataset\n",
    "df_ml_ready = final_assembler.transform(df_scaled)\n",
    "\n",
    "# Select only necessary columns for ML\n",
    "ml_dataset = df_ml_ready.select(\n",
    "    'transaction_id',\n",
    "    'price',  # target variable\n",
    "    'log_price',  # alternative target (for regression)\n",
    "    'features',  # feature vector\n",
    "    'date_of_transfer',\n",
    "    'postcode',\n",
    "    'property_type',\n",
    "    'county',\n",
    "    'town_city'\n",
    ")\n",
    "\n",
    "print(\"Final ML dataset created with columns:\")\n",
    "for col in ml_dataset.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {ml_dataset.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0abc4257-b77e-4755-8e89-5ee32aa60234",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Preparation Summary and Save"
    }
   },
   "outputs": [],
   "source": [
    "# Create train/validation/test splits\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Add random column for splitting\n",
    "ml_dataset_with_split = ml_dataset.withColumn('rand', rand(seed=42))\n",
    "\n",
    "# Create splits: 70% train, 15% validation, 15% test\n",
    "train_data = ml_dataset_with_split.filter(col('rand') < 0.7).drop('rand')\n",
    "val_data = ml_dataset_with_split.filter((col('rand') >= 0.7) & (col('rand') < 0.85)).drop('rand')\n",
    "test_data = ml_dataset_with_split.filter(col('rand') >= 0.85).drop('rand')\n",
    "\n",
    "print(\"=== DATA PREPARATION SUMMARY ===\")\n",
    "print(f\"Training set: {train_data.count():,} records ({train_data.count()/ml_dataset.count()*100:.1f}%)\")\n",
    "print(f\"Validation set: {val_data.count():,} records ({val_data.count()/ml_dataset.count()*100:.1f}%)\")\n",
    "print(f\"Test set: {test_data.count():,} records ({test_data.count()/ml_dataset.count()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== FEATURES CREATED ===\")\n",
    "print(\"Date Features: year, month, quarter, season, day_of_week, is_weekend\")\n",
    "print(\"Price Features: log_price, price_category, price_type_ratio\")\n",
    "print(\"Location Features: postcode components, is_london, is_major_city, county_group\")\n",
    "print(\"Property Features: is_new_property, is_freehold, property_size_score, address_completeness\")\n",
    "print(\"Categorical Encoding: One-hot encoded for all categorical variables\")\n",
    "print(\"Numerical Scaling: StandardScaler applied to continuous features\")\n",
    "\n",
    "# Save the prepared datasets (optional)\n",
    "# Uncomment the lines below to save the datasets\n",
    "# train_data.write.mode('overwrite').saveAsTable('caio_moreno.uk_house_advisor_bot.ml_train_data')\n",
    "# val_data.write.mode('overwrite').saveAsTable('caio_moreno.uk_house_advisor_bot.ml_val_data')\n",
    "# test_data.write.mode('overwrite').saveAsTable('caio_moreno.uk_house_advisor_bot.ml_test_data')\n",
    "\n",
    "print(\"\\n✅ Data preparation completed! Ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f12b68-8842-4431-acd5-f72124980ac5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dataset Statistics and Exploration"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset statistics\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "\n",
    "# Check data types and null counts\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "# Calculate null counts for each column (handle different data types)\n",
    "null_counts = []\n",
    "for c in df.columns:\n",
    "    if isinstance(df.schema[c].dataType, (DoubleType, FloatType)):\n",
    "        null_count = df.select(count(when(col(c).isNull() | isnan(col(c)), c)).alias(c))\n",
    "    else:\n",
    "        null_count = df.select(count(when(col(c).isNull(), c)).alias(c))\n",
    "    null_counts.append(null_count)\n",
    "\n",
    "# Combine all null counts\n",
    "from functools import reduce\n",
    "null_summary = reduce(lambda df1, df2: df1.union(df2), null_counts)\n",
    "print(\"\\nNull counts by column:\")\n",
    "null_summary.show()\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"\\nBasic statistics for price column:\")\n",
    "df.select(\"price\").describe().show()\n",
    "\n",
    "# Check unique values for categorical columns\n",
    "print(\"\\nUnique values in key categorical columns:\")\n",
    "print(f\"Property types: {df.select('property_type').distinct().count()}\")\n",
    "print(f\"Counties: {df.select('county').distinct().count()}\")\n",
    "print(f\"Towns/Cities: {df.select('town_city').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7565876f-4a25-4fd2-bb7f-912e843dec88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1.Train_Basic_ML_Model_DB_DS_Agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
